"""
RL Training Guide and Setup

This module provides guidance and utilities for training the agent using:
- SFT (Supervised Fine-Tuning)
- DPO (Direct Preference Optimization)
- GRPO (Group Relative Policy Optimization)

NOTE: Actual training requires ML infrastructure (GPUs, training frameworks).
This module provides the data preparation and configuration.

Recommended Training Stack:
- Model: Qwen2.5, LLaMA 3.2, or Mistral (open-source alternatives to GPT-4)
- Framework: HuggingFace transformers + trl (Transformer Reinforcement Learning)
- Hardware: 1-4 GPUs (A100, H100, or A6000)
- Platform: Local cluster, AWS, Azure, or GCP

Training Pipeline:
1. Collect interactions → (Phase 4: InteractionLogger)
2. Build datasets → (Phase 4: DatasetBuilder)
3. Train SFT model → (This guide)
4. Train DPO/GRPO model → (This guide)
5. Deploy fine-tuned model → (Replace OpenAI calls)
"""

import logging
from typing import Dict, List, Optional
from pathlib import Path

logger = logging.getLogger(__name__)


class TrainingConfig:
    """
    Configuration for RL training.

    Based on research papers (Orion, DeepSeek-Prover-V2, Tool-R1).
    """

    # Model selection
    base_model: str = "Qwen/Qwen2.5-7B-Instruct"  # Or LLaMA-3.2-8B
    model_max_length: int = 4096

    # SFT (Supervised Fine-Tuning)
    sft_epochs: int = 3
    sft_batch_size: int = 4
    sft_learning_rate: float = 2e-5
    sft_warmup_ratio: float = 0.1

    # DPO (Direct Preference Optimization)
    dpo_beta: float = 0.1  # KL penalty coefficient
    dpo_epochs: int = 1
    dpo_batch_size: int = 4
    dpo_learning_rate: float = 1e-6

    # GRPO (Group Relative Policy Optimization)
    grpo_group_size: int = 4
    grpo_epochs: int = 1
    grpo_learning_rate: float = 1e-6

    # General
    gradient_accumulation_steps: int = 4
    max_grad_norm: float = 1.0
    weight_decay: float = 0.01
    save_steps: int = 500
    eval_steps: int = 500

    def to_dict(self) -> Dict:
        """Convert to dict for saving"""
        return {k: v for k, v in self.__dict__.items() if not k.startswith('_')}


def generate_sft_training_script(
    dataset_path: str,
    output_dir: str,
    config: Optional[TrainingConfig] = None
) -> str:
    """
    Generate Python script for SFT training.

    Args:
        dataset_path: Path to SFT dataset (jsonl)
        output_dir: Output directory for fine-tuned model
        config: Training configuration

    Returns:
        Python training script as string
    """
    if config is None:
        config = TrainingConfig()

    script = f'''"""
Supervised Fine-Tuning (SFT) Script

Generated by agentic-backend Phase 4.
Trains model on high-quality agent interactions.

Requirements:
    pip install transformers trl datasets accelerate bitsandbytes
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import SFTTrainer
from datasets import load_dataset

# Configuration
BASE_MODEL = "{config.base_model}"
DATASET_PATH = "{dataset_path}"
OUTPUT_DIR = "{output_dir}"

# Load model and tokenizer
print(f"Loading model: {{BASE_MODEL}}")
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(
    BASE_MODEL,
    trust_remote_code=True
)
tokenizer.pad_token = tokenizer.eos_token

# Load dataset
print(f"Loading dataset: {{DATASET_PATH}}")
dataset = load_dataset("json", data_files=DATASET_PATH, split="train")

# Format function
def format_sample(example):
    """Convert to training format"""
    messages = example["messages"]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False
    )
    return {{"text": text}}

dataset = dataset.map(format_sample)

# Training arguments
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs={config.sft_epochs},
    per_device_train_batch_size={config.sft_batch_size},
    gradient_accumulation_steps={config.gradient_accumulation_steps},
    learning_rate={config.sft_learning_rate},
    warmup_ratio={config.sft_warmup_ratio},
    logging_steps=10,
    save_steps={config.save_steps},
    eval_steps={config.eval_steps},
    bf16=True,
    max_grad_norm={config.max_grad_norm},
    weight_decay={config.weight_decay},
    save_total_limit=3,
    load_best_model_at_end=True,
    report_to="tensorboard"
)

# Trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    max_seq_length={config.model_max_length},
    dataset_text_field="text"
)

# Train
print("Starting SFT training...")
trainer.train()

# Save
print(f"Saving model to {{OUTPUT_DIR}}")
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("Training complete!")
'''

    return script


def generate_dpo_training_script(
    dataset_path: str,
    sft_model_path: str,
    output_dir: str,
    config: Optional[TrainingConfig] = None
) -> str:
    """
    Generate Python script for DPO training.

    Args:
        dataset_path: Path to DPO dataset (jsonl)
        sft_model_path: Path to SFT model (base for DPO)
        output_dir: Output directory for DPO model
        config: Training configuration

    Returns:
        Python training script as string
    """
    if config is None:
        config = TrainingConfig()

    script = f'''"""
Direct Preference Optimization (DPO) Script

Generated by agentic-backend Phase 4.
Trains model to prefer high-quality responses over low-quality ones.

Requirements:
    pip install transformers trl datasets accelerate bitsandbytes
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import DPOTrainer
from datasets import load_dataset

# Configuration
BASE_MODEL = "{sft_model_path}"  # Start from SFT model
DATASET_PATH = "{dataset_path}"
OUTPUT_DIR = "{output_dir}"

# Load model and tokenizer
print(f"Loading SFT model: {{BASE_MODEL}}")
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(
    BASE_MODEL,
    trust_remote_code=True
)
tokenizer.pad_token = tokenizer.eos_token

# Load dataset
print(f"Loading DPO dataset: {{DATASET_PATH}}")
dataset = load_dataset("json", data_files=DATASET_PATH, split="train")

# Training arguments
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs={config.dpo_epochs},
    per_device_train_batch_size={config.dpo_batch_size},
    gradient_accumulation_steps={config.gradient_accumulation_steps},
    learning_rate={config.dpo_learning_rate},
    logging_steps=10,
    save_steps={config.save_steps},
    bf16=True,
    max_grad_norm={config.max_grad_norm},
    weight_decay={config.weight_decay},
    save_total_limit=3,
    report_to="tensorboard"
)

# DPO Trainer
trainer = DPOTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    beta={config.dpo_beta},  # KL penalty
    max_prompt_length=512,
    max_length={config.model_max_length}
)

# Train
print("Starting DPO training...")
trainer.train()

# Save
print(f"Saving DPO model to {{OUTPUT_DIR}}")
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("DPO training complete!")
'''

    return script


def save_training_script(
    script_content: str,
    filepath: str
) -> bool:
    """
    Save training script to file.

    Args:
        script_content: Script content
        filepath: Output file path

    Returns:
        True if saved successfully
    """
    try:
        path = Path(filepath)
        path.parent.mkdir(parents=True, exist_ok=True)

        with open(filepath, 'w') as f:
            f.write(script_content)

        # Make executable
        path.chmod(0o755)

        logger.info(f"Saved training script to {filepath}")
        return True

    except Exception as e:
        logger.error(f"Failed to save training script: {e}")
        return False


def get_training_recommendations() -> Dict[str, str]:
    """
    Get training recommendations based on dataset size.

    Returns:
        Dict with recommendations
    """
    return {
        "small_dataset": """
Dataset Size: < 100 samples
Recommendation: Not ready for training yet.
Action: Collect more interactions (target: 1000+)
Alternative: Use few-shot prompting instead of fine-tuning
        """,

        "medium_dataset": """
Dataset Size: 100-1000 samples
Recommendation: SFT only
Steps:
  1. Build SFT dataset (min_quality=0.8)
  2. Train on Qwen2.5-7B or LLaMA-3.2-8B
  3. Evaluate on held-out test set
  4. Deploy if performance improves
        """,

        "large_dataset": """
Dataset Size: 1000-10000 samples
Recommendation: SFT → DPO pipeline
Steps:
  1. Build SFT dataset (min_quality=0.8)
  2. Train SFT model
  3. Build DPO dataset from pairs (min_diff=0.2)
  4. Train DPO on top of SFT model
  5. Evaluate both models
  6. Deploy best performer
        """,

        "very_large_dataset": """
Dataset Size: 10000+ samples
Recommendation: SFT → GRPO pipeline
Steps:
  1. Build SFT dataset (min_quality=0.8)
  2. Train SFT model
  3. Build GRPO dataset (groups of 4)
  4. Train GRPO model
  5. Continuous improvement loop:
     - Collect new interactions
     - Add to training data
     - Retrain periodically
        """
    }


# ============================================================================
# Training Workflow Documentation
# ============================================================================

TRAINING_WORKFLOW = """
Complete RL Training Workflow
==============================

Phase 4 enables self-improving agents through this workflow:

1. DATA COLLECTION (Automatic)
   ✓ All interactions logged with quality scores
   ✓ Tool usage tracked
   ✓ Performance metrics recorded
   → Handled by InteractionLogger

2. DATASET BUILDING (On-demand)
   ✓ Filter high-quality interactions (>= 0.8)
   ✓ Create SFT dataset (Q&A pairs)
   ✓ Create DPO dataset (preference pairs)
   ✓ Create GRPO dataset (grouped responses)
   → Handled by DatasetBuilder

3. MODEL TRAINING (External)
   ✓ SFT: Fine-tune on high-quality demonstrations
   ✓ DPO: Learn from preferences
   ✓ GRPO: Group-relative optimization
   → Use generated training scripts
   → Requires: GPU infrastructure, ML frameworks

4. EVALUATION (Post-training)
   ✓ Test on held-out dataset
   ✓ Compare to base model
   ✓ A/B test in production
   → Use quality scoring from Phase 1

5. DEPLOYMENT (If improved)
   ✓ Replace OpenAI API calls with fine-tuned model
   ✓ Update app/config.py with model path
   ✓ Monitor performance
   → Continue collecting data for next iteration

Recommended Schedule:
- Week 1-4: Collect 1000+ interactions
- Week 5: Build SFT dataset, generate training script
- Week 6-7: Train SFT model (external GPU cluster)
- Week 8: Evaluate and deploy
- Repeat: Continuous improvement cycle

Cost Estimates:
- Data collection: Free (automatic)
- Training (AWS p3.2xlarge): $3/hour × 10 hours = $30
- Training (local A100): Hardware cost
- Inference: Cheaper than OpenAI (open-source models)

ROI: Fine-tuned models are:
- 20-40% more accurate on your specific tasks
- 50-80% cheaper for inference
- Fully controlled (no API rate limits)
"""
